import torch

class Config:
    """
    Transformer模型和训练的超参数配置类
    
    用途:
    - 集中管理所有模型和训练参数
    - 便于调整实验设置
    - 确保参数一致性
    """
    
    # ======================
    # 模型架构参数 (根据论文推荐值)
    # ======================
    
    # 词嵌入维度 (d_model)
    # 选择理由: 
    #   - 控制词向量和所有隐藏层的维度
    #   - 512是原始论文推荐值，平衡表达能力和计算效率
    d_model = 512
    
    # 注意力头数 (num_heads)
    # 选择理由:
    #   - 允许模型同时关注不同表示子空间
    #   - 512/8=64，每个头有足够的表达能力
    num_heads = 8
    
    # 编码器/解码器层数 (num_layers)
    # 选择理由:
    #   - 决定模型深度
    #   - 6层是原始论文推荐值，在翻译任务中表现最佳
    num_layers = 6
    
    # 前馈网络隐藏层维度 (d_ff)
    # 选择理由:
    #   - 提供非线性变换能力
    #   - 2048是原始论文推荐值，通常是d_model的4倍
    d_ff = 2048
    
    # Dropout概率
    # 选择理由:
    #   - 防止过拟合的正则化技术
    #   - 0.1是Transformer常用值，避免过度正则化
    dropout = 0.1
    
    # 最大序列长度 (max_seq_len)
    # 选择理由:
    #   - 位置编码支持的最大长度
    #   - 100适用于大多数句子级任务
    max_seq_len = 100
    
    # 填充标记索引 (pad_idx)
    # 选择理由:
    #   - 统一表示填充位置
    #   - 通常设为0，作为词汇表的第一个索引
    pad_idx = 0
    
    # ======================
    # 训练参数 (根据经验值)
    # ======================
    
    # 批次大小 (batch_size)
    # 选择理由:
    #   - 每次迭代处理的样本数
    #   - 64是常用值，平衡内存使用和梯度稳定性
    batch_size = 64
    
    # 训练轮数 (epochs)
    # 选择理由:
    #   - 整个数据集遍历次数
    #   - 10轮作为起点，实际根据验证损失调整
    epochs = 10
    
    # 学习率 (lr)
    # 选择理由:
    #   - 控制参数更新步长
    #   - 0.0001(1e-4)是Transformer常用初始值
    lr = 0.0001
    
    # 梯度裁剪阈值 (clip)
    # 选择理由:
    #   - 防止梯度爆炸
    #   - 1.0是Transformer论文推荐值
    clip = 1.0
    
    # 计算设备 (device)
    # 选择理由:
    #   - 自动选择GPU(如果可用)或CPU
    #   - 优先使用GPU加速训练
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # ======================
    # 词汇表参数 (根据数据集)
    # ======================
    
    # 源语言词汇表大小 (src_vocab_size)
    # 注意: 实际值在数据加载时确定
    # 用途: 词嵌入层的输入维度
    src_vocab_size = None
    
    # 目标语言词汇表大小 (tgt_vocab_size)
    # 注意: 实际值在数据加载时确定
    # 用途: 输出层的维度
    tgt_vocab_size = None
    
    # ======================
    # 训练辅助参数
    # ======================
    
    # 模型保存路径 (model_save_path)
    # 用途: 训练完成后保存模型权重
    model_save_path = "transformer_model.pth"
    
    # 日志频率 (log_freq)
    # 选择理由:
    #   - 每100批次打印一次训练进度
    #   - 避免过多日志输出
    log_freq = 100

# 可选：创建配置实例的辅助函数
def get_config():
    """
    获取配置实例
    
    返回:
    Config: 配置对象实例
    
    用途:
    - 统一获取配置的方式
    - 便于未来扩展配置加载逻辑
    """
    return Config()